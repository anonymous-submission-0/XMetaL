##################### default args #####################
common:
  thread_num: 3 # load and process data with multi-process
  seed: 666 # init seed for numpy/torch/random/scipy and so on
  device: 0 # GPU device. if None, as CPU

  # bsz=4
  init_weights: /mnt/wanyao/.meta_learning/code_search_net/checkpoints/maml_p1.0/high_resource/maml_ft_1.0_Adam(0.0004)/tok8path-bs4-lr0.0004-attndot-pointerTrue-ttFTTrainer-ep2.pt
#  |     B1 |     B2 |   PycocoB4 |   Meteor |   PycocoMeteor |    R1 |     R2 |     RL |   PycocoRL |   Cider |
#  |--------|--------|------------|----------|----------------|-------|--------|--------|------------|---------|
#  | 0.1542 | 0.0481 |     0.1425 |   0.0837 |         0.0737 | 0.218 | 0.0434 | 0.1958 |     0.1816 |  0.4116 |

  task: maml
  init_bound: 1e-1
  init_normal_std: 1e-4
  result_table_format: 'github' # latex (https://pypi.org/project/tabulate/)

dataset:
  dataset_dir: /mnt/wanyao/.meta_learning/code_search_net/data
  # epoch 857, bleu1=23.16, cider=50.47
  save_dir: /mnt/wanyao/.meta_learning/code_search_net/checkpoints/maml_p1.0/high_resource

  ##################### dataset and other args #####################
  # dict files(tok, tree, path sbt, sbt2, code_tokens, docstring_tokens, method)
  # load from code_modalities

  tree_leaf_subtoken: True
  src_portion: ~ # only for evaluate meta-learner, using part data is quick
  portion: ~ # only for evaluate meta-learner, using part data is quick
  leaf_path_k: 30
  source:
    dataset_lng:
      - 'python'
      - 'javascript'
      - 'php'
      - 'java'
      - 'go'
      - 'ruby'

    mode:
      - 'train'

  target:
    dataset_lng:
      - 'nix'

    mode:
      - 'train'
      - 'valid'
      - 'test'


training:
  ##################### model args #####################
  #  summarization: tok, ast, path sbt, sbt2
  #  retrieval: code_tokens, docstring_tokens, method
  code_modalities:
    - tok
    - path

  train_epoch: 10 # for maml
  batch_size: 4 # in maml scenario, we recommend to use a small number as batch_size
  log_interval: 20 # write log info per log_interval iteration

  # network: encoder
  rnn_type: 'LSTM' # RNN type: 'GRU','LSTM', LSTM as default
  rnn_layer_num: 1 # RNN layer num
  rnn_hidden_size: 512 # RNN hidden size
  rnn_bidirectional: True

  embed_size: 300 # word-embedding size
  embed_pooling: ~

  tree_lstm_cell_type: 'nary' # DGL tree LSTM cell, if nary -> TreeLSTMCell, else    -> ChildSumTreeLSTMCell
  code_modal_transform: False # code modalities transform with FCs

  # encoder RNN's output as decoder RNN's hidden_state(h, c)
  # 1) None, for zero_init as decoder RNN's hidden_state
  # 2) h, for only encoder RNN's output as decoder RNN's hidden_state(h)
  # 3) c, for only encoder RNN's output as decoder RNN's hidden_state(c)
  # 3) hc, for encoder RNN's output as decoder RNN's hidden_state(h, c)
  enc_hc2dec_hc: 'h'

  # network: decoder
  attn_type: 'dot' # ~, general, dot, mlp, intra
  attn_unit: 512
  self_attn_size: 50
  intra_attn_eps: 1e-12
  pointer: True # pointer-generator
  max_predict_length: 30 # max generation length for decoder
  dropout: 0.2 # dropout
  decoder_input_feed: False # （固定，不要调）True, False


# inference
testing:
  beam_size: 10
  max_predict_length: ~ # max generation length for decoder

  metrics:
    - 'bleu'
    #    - 'meteor'
    #    - 'rouge'
    - 'cider'

# supervised learning
sl:
  optim: 'Adam' # （固定，不要调）'Adam', 'AdamW', 'Adagrad', 'RMSprop', 'SGD'

  # default
  lr: 4e-4
  lr_gamma: 0.5 # （固定，不要调）0.1
  lr_milestones: # 默认最优，epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5

  warmup_factor: 0.2 #（固定，不要调）
  warmup_epochs: -1 # （固定，不要调）-1: no warmup
  max_grad_norm: -1 # （固定，不要调）-1: no gradient clips  1.0, 10, 20, 100

